{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import logging\n",
    "import shutil\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from random import shuffle, sample\n",
    "from typing import Dict, Any, Optional\n",
    "import json\n",
    "from functools import partial, update_wrapper\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import ase\n",
    "from ase.db import connect\n",
    "from ase.md.velocitydistribution import MaxwellBoltzmannDistribution\n",
    "from fff.learning.gc.ase import SchnetCalculator\n",
    "from fff.learning.gc.functions import GCSchNetForcefield\n",
    "from fff.learning.gc.models import SchNet, load_pretrained_model\n",
    "from fff.learning.util.messages import TorchMessage\n",
    "from fff.sampling.md import MolecularDynamics\n",
    "from fff.simulation import run_calculator, _run_calculator\n",
    "from fff.simulation.utils import read_from_string, write_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## util functions\n",
    "## dataclass\n",
    "@dataclass\n",
    "class Trajectory:\n",
    "    \"\"\"Tracks the state of searching along individual trajectories\n",
    "\n",
    "    We mark the starting point, the last point produced from sampling,\n",
    "    and the last point we produced that has been validated\n",
    "    \"\"\"\n",
    "    id: int  # ID number of the\n",
    "    starting: ase.Atoms  # Starting point of the trajectory\n",
    "    current_timestep = 0  # How many timesteps have been used so far\n",
    "    last_validated: ase.Atoms = None  # Last validated point on the trajectory\n",
    "    current: ase.Atoms = None  # Last point produced along the trajectory\n",
    "    last_run_length: int = 0  # How long between current and last_validated\n",
    "    name: str = None  # Name of the trajectory\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.last_validated = self.current = self.starting\n",
    "\n",
    "    def update_current_structure(self, strc: ase.Atoms, run_length: int):\n",
    "        \"\"\"Update the structure that has yet to be updated\n",
    "\n",
    "        Args:\n",
    "            strc: Structure produced by sampling\n",
    "            run_length: How many timesteps were performed in sampling run\n",
    "        \"\"\"\n",
    "        self.current = strc.copy()\n",
    "        self.last_run_length = run_length\n",
    "\n",
    "    def set_validation(self, success: bool):\n",
    "        \"\"\"Set whether the trajectory was successfully validated\n",
    "\n",
    "        Args:\n",
    "            success: Whether the validation was successful\n",
    "        \"\"\"\n",
    "        if success:\n",
    "            self.last_validated = self.current  # Move the last validated forward\n",
    "            self.current_timestep += self.last_run_length\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SimulationTask:\n",
    "    atoms: ase.Atoms  # Structure to be run\n",
    "    traj_id: int  # Which trajectory this came from\n",
    "    ml_eng: float  # Energy predicted from machine learning model\n",
    "    ml_std: Optional[float] = None  # Uncertainty of the model\n",
    "    \n",
    "# Apply wrappers to functions that will be used to fix certain requirements\n",
    "def _wrap(func, **kwargs):\n",
    "    out = partial(func, **kwargs)\n",
    "    update_wrapper(out, func)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path and varaibles\n",
    "## pash on wsl\n",
    "multisite_path = \"/home/yxx/work/project/colmena/multisite_\"\n",
    "training_set = multisite_path + \\\n",
    "    \"/data/forcefields/starting-model/initial-database.db\"\n",
    "model_path = multisite_path + \"/data/forcefields/starting-model/starting-model\"\n",
    "search_path = training_set\n",
    "out_dir = Path(multisite_path) / f\"evt/idea_simple_validate/temp\"\n",
    "fff_temp = Path(multisite_path) / f\"evt/idea_simple_validate/fff_temp\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "starting_model = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "num_epochs = 16\n",
    "huber_deltas = (1, 10)\n",
    "sampler_kwargs = {'device': \"cpu\", 'timestep': 0.1, 'log_interval': 10}\n",
    "sampler = MolecularDynamics()\n",
    "n_models = 1\n",
    "n_qc_workers = 8\n",
    "min_run_length = 200\n",
    "max_run_length = 2000\n",
    "energy_tolerance = 0.1\n",
    "infer_chunk_size = 100\n",
    "\n",
    "# search space queue\n",
    "to_audit: dict[int, Trajectory] = {}  # Trajectories that need to be audited\n",
    "audit_results: deque[float] = deque(maxlen=50)  # Results of the last 50 audits\n",
    "task_queue_audit = []\n",
    "inference_pool = []\n",
    "\n",
    "# Prepare the initial model\n",
    "StartModelMessage = TorchMessage(starting_model)\n",
    "ActiveModelMessage = SchnetCalculator(starting_model)\n",
    "\n",
    "# get model\n",
    "schnet = GCSchNetForcefield(starting_model)\n",
    "\n",
    "# copy training data\n",
    "train_path = out_dir / \"train.db\"\n",
    "shutil.copyfile(training_set, train_path)\n",
    "\n",
    "# Load in the search space\n",
    "with connect(search_path) as db:\n",
    "    search_space = [Trajectory(i, x.toatoms(), name=x.get(\n",
    "        'filename', f'traj-{i}')) for i, x in enumerate(db.select(''))]\n",
    "    shuffle(search_space)\n",
    "    search_space = deque(search_space)\n",
    "    \n",
    "# Load in the training dataset\n",
    "with connect(train_path) as db:\n",
    "    all_examples = np.array([x.toatoms() for x in db.select(\"\")], dtype=object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train\n",
      "end train at6.027985334396362\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "Initialized temperature to 100K\n",
      "100\n",
      "1900\n"
     ]
    }
   ],
   "source": [
    "## task define\n",
    "# train task\n",
    "\n",
    "    \n",
    "# Prepare the dataset\n",
    "train_sets = []\n",
    "valid_sets = []\n",
    "n_train = int(len(all_examples) * 0.9)\n",
    "for _ in range(n_models):\n",
    "    shuffle(all_examples)\n",
    "    train_sets.append(all_examples[:n_train])\n",
    "    valid_sets.append(all_examples[n_train:])\n",
    "\n",
    "# store model and log\n",
    "model_msgs = []\n",
    "train_logs = []\n",
    "\n",
    "# train model function\n",
    "my_train_schnet = _wrap(schnet.train, num_epochs=num_epochs, device='cuda',\n",
    "                        patience=8, reset_weights=False,\n",
    "                        huber_deltas=huber_deltas)\n",
    "\n",
    "# train model\n",
    "print(\"start train\")\n",
    "start_time = time.time()\n",
    "for i in range(0, 1):\n",
    "    for i, train_set in enumerate(valid_sets):\n",
    "        model_msg, train_log = my_train_schnet(\n",
    "            model_msg=StartModelMessage, train_data=train_set, valid_data=valid_sets[i])\n",
    "        model_msgs.append(model_msg)\n",
    "        train_logs.append(train_log)\n",
    "\n",
    "    # store model\n",
    "    # now we just test one model\n",
    "    model_save_path = out_dir / \"model.pth\"\n",
    "    with open(model_save_path, 'wb') as fp:\n",
    "        torch.save(model_msgs[0].get_model(), fp)\n",
    "    # Save the training data\n",
    "    with open(out_dir / 'training-history.json', 'a') as fp:\n",
    "        print(json.dumps(train_logs[0].to_dict(orient='list')), file=fp)\n",
    "    # save model message\n",
    "    with open(out_dir / 'model_msgs.pkl', 'wb') as fp:\n",
    "        pickle.dump(model_msgs, fp)\n",
    "\n",
    "    active_model_proxy = SchnetCalculator(model_msgs[0].get_model())\n",
    "    StartModelMessage = TorchMessage(model_msgs[0].get_model()) \n",
    "    \n",
    "print(\"end train at\"+str(time.time()-start_time))\n",
    "#-------------------------------------------------------------------------------\n",
    "# sampling task\n",
    "# use model sampling\n",
    "my_run_dynamics = _wrap(sampler.run_sampling, **sampler_kwargs)\n",
    "\n",
    "# filter small atjjjoms\n",
    "filtered_search_space = deque()\n",
    "for i in range(len(search_space)):\n",
    "    tt = search_space.popleft()\n",
    "    if (len(tt.starting)<50):\n",
    "        filtered_search_space.append(tt)\n",
    "shuffle(filtered_search_space)\n",
    "search_space=filtered_search_space\n",
    "# sampling tasks loop, prepare task data\n",
    "for i in range(0, 100):\n",
    "    # Pick the next eligible trajectory and start from the last validated structure\n",
    "    trajectory = search_space.popleft()\n",
    "    starting_point = trajectory.starting\n",
    "\n",
    "    # Initialize the structure if need be\n",
    "    if trajectory.current_timestep == 0:\n",
    "        MaxwellBoltzmannDistribution(starting_point, temperature_K=100)\n",
    "        print('Initialized temperature to 100K')\n",
    "    # Add the structure to a list of those being validated\n",
    "    to_audit[trajectory.id] = trajectory\n",
    "\n",
    "    # Determine the run length based on observations of errors\n",
    "    run_length = min_run_length\n",
    "    if len(audit_results) > n_qc_workers:\n",
    "        # Predict run length given audit error\n",
    "        error_per_step = np.median(audit_results)\n",
    "        target_error = energy_tolerance * 2\n",
    "        estimated_run_length = int(target_error / error_per_step)\n",
    "        print(\n",
    "            f'Estimated run length of {estimated_run_length} steps to have an error of {target_error:.3f} eV/atom')\n",
    "        # Keep to within the user-defined bounds\n",
    "        run_length = max(min_run_length, min(\n",
    "            max_run_length, estimated_run_length))\n",
    "\n",
    "    # do sampling\n",
    "    audit, traj = my_run_dynamics(\n",
    "        atoms=starting_point, steps=run_length, calc=active_model_proxy)\n",
    "    # print(audit)\n",
    "    # print(len(traj))\n",
    "    # add to list\n",
    "    to_audit[trajectory.id].update_current_structure(audit, run_length)\n",
    "    task_queue_audit.append(SimulationTask(\n",
    "        atoms=traj[-1], traj_id=trajectory.id, ml_eng=traj[-1].get_potential_energy()))\n",
    "    inference_pool.extend(traj)\n",
    "\n",
    "print(len(task_queue_audit))\n",
    "print(len(inference_pool))\n",
    "## pickletask_queue_audit and inference_pool\n",
    "with open(out_dir / 'task_queue_audit.pkl', 'wb') as fp:\n",
    "    pickle.dump(task_queue_audit, fp)\n",
    "with open(out_dir / 'inference_pool.pkl', 'wb') as fp:\n",
    "    pickle.dump(inference_pool, fp)\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# simulation task\n",
    "# get simulation task data\n",
    "# to_run = task_queue_audit[-1]\n",
    "\n",
    "def simulation_task(to_run:SimulationTask,tempdir:str=\"./ffftemp\",num_threads:int=8):\n",
    "    \"\"\"Run a simulation task\n",
    "\n",
    "    Args:\n",
    "        to_run: SimulationTask to be run\n",
    "        tempdir: Temporary directory for running simulations\n",
    "        num_threads: Number of threads to use for simulation\n",
    "    \"\"\"\n",
    "    calc = dict(calc='psi4', method='pbe0-d3', basis='aug-cc-pvdz', num_threads=num_threads)\n",
    "    ml_eng = to_run.ml_eng\n",
    "    atoms = to_run.atoms\n",
    "    atoms.set_center_of_mass([0, 0, 0])\n",
    "    xyz = write_to_string(atoms, 'xyz')\n",
    "    value = _run_calculator(xyz, calc, tempdir)\n",
    "    atoms = read_from_string(value, 'json')\n",
    "    dft_energy = atoms.get_potential_energy()\n",
    "    diff_energy = abs(dft_energy - ml_eng) / len(atoms)\n",
    "    return diff_energy\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# inference task\n",
    "inference_batch = inference_pool[0:100]\n",
    "inference_proxies: list[None] = [None] * n_models\n",
    "inference_proxies[0] = SchnetCalculator(model_msgs[0])\n",
    "my_eval_schnet = _wrap(schnet.evaluate, device='cuda')\n",
    "my_eval_schnet(\"/home/yxx/work/project/colmena/multisite_/evt/idea_simple_validate/temp/model.pth\",inference_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end train at6.07136869430542\n",
      "end sampling at2.787306547164917\n",
      "  Threads set to 8 by Python driver.\n",
      "  Threads set to 8 by Python driver.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Calculation failed: \nFatal Error: Matrix::power: C_DSYEV failed\nError occurred in file: /scratch/psilocaluser/conda-builds/psi4-multiout_1657298395608/work/psi4/src/psi4/libmints/matrix.cc on line: 2330\nThe most recent 5 function calls were:\n\npsi::FittingMetric::form_eig_inverse(double)\npsi::DiskDFJK::preiterations()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/work/project/colmena/multisite_/finetuning-surrogates/fff/simulation/__init__.py:56\u001b[0m, in \u001b[0;36m_run_calculator\u001b[0;34m(xyz, calc, temp_path)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     atoms\u001b[39m.\u001b[39;49mget_forces()\n\u001b[1;32m     57\u001b[0m     atoms\u001b[39m.\u001b[39mget_potential_energy()\n",
      "File \u001b[0;32m~/miniconda3/envs/multisite/lib/python3.9/site-packages/ase/atoms.py:788\u001b[0m, in \u001b[0;36mAtoms.get_forces\u001b[0;34m(self, apply_constraint, md)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAtoms object has no calculator.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 788\u001b[0m forces \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_calc\u001b[39m.\u001b[39;49mget_forces(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    790\u001b[0m \u001b[39mif\u001b[39;00m apply_constraint:\n\u001b[1;32m    791\u001b[0m     \u001b[39m# We need a special md flag here because for MD we want\u001b[39;00m\n\u001b[1;32m    792\u001b[0m     \u001b[39m# to skip real constraints but include special \"constraints\"\u001b[39;00m\n\u001b[1;32m    793\u001b[0m     \u001b[39m# Like Hookean.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/multisite/lib/python3.9/site-packages/ase/calculators/abc.py:23\u001b[0m, in \u001b[0;36mGetPropertiesMixin.get_forces\u001b[0;34m(self, atoms)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_forces\u001b[39m(\u001b[39mself\u001b[39m, atoms\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_property(\u001b[39m'\u001b[39;49m\u001b[39mforces\u001b[39;49m\u001b[39m'\u001b[39;49m, atoms)\n",
      "File \u001b[0;32m~/miniconda3/envs/multisite/lib/python3.9/site-packages/ase/calculators/calculator.py:737\u001b[0m, in \u001b[0;36mCalculator.get_property\u001b[0;34m(self, name, atoms, allow_calculation)\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 737\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcalculate(atoms, [name], system_changes)\n\u001b[1;32m    739\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults:\n\u001b[1;32m    740\u001b[0m     \u001b[39m# For some reason the calculator was not able to do what we want,\u001b[39;00m\n\u001b[1;32m    741\u001b[0m     \u001b[39m# and that is OK.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/multisite/lib/python3.9/site-packages/ase/calculators/psi4.py:179\u001b[0m, in \u001b[0;36mPsi4.calculate\u001b[0;34m(self, atoms, properties, system_changes, symmetry)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mforces\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m properties:\n\u001b[0;32m--> 179\u001b[0m     grad, wf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpsi4\u001b[39m.\u001b[39;49mdriver\u001b[39m.\u001b[39;49mgradient(\u001b[39m'\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mformat(method, basis),\n\u001b[1;32m    180\u001b[0m                                          return_wfn\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    181\u001b[0m     \u001b[39m# energy comes for free\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/multisite/lib/python3.9/site-packages/psi4/driver/driver.py:619\u001b[0m, in \u001b[0;36mgradient\u001b[0;34m(name, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mw/EFP\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(molecule, \u001b[39m\"\u001b[39m\u001b[39mEFP\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m pp\u001b[39m.\u001b[39mpformat(molecule\u001b[39m.\u001b[39mto_dict()))\n\u001b[0;32m--> 619\u001b[0m wfn \u001b[39m=\u001b[39m procedures[\u001b[39m'\u001b[39;49m\u001b[39mgradient\u001b[39;49m\u001b[39m'\u001b[39;49m][lowername](lowername, molecule\u001b[39m=\u001b[39;49mmolecule, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    620\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReturn gradient(): \u001b[39m\u001b[39m{\u001b[39;00mcore\u001b[39m.\u001b[39mvariable(\u001b[39m'\u001b[39m\u001b[39mCURRENT ENERGY\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/multisite/lib/python3.9/site-packages/psi4/driver/procrouting/proc.py:90\u001b[0m, in \u001b[0;36mselect_scf_gradient\u001b[0;34m(name, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[39mreturn\u001b[39;00m func(name, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/multisite/lib/python3.9/site-packages/psi4/driver/procrouting/proc.py:2550\u001b[0m, in \u001b[0;36mrun_scf_gradient\u001b[0;34m(name, **kwargs)\u001b[0m\n\u001b[1;32m   2549\u001b[0m \u001b[39mif\u001b[39;00m ref_wfn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2550\u001b[0m     ref_wfn \u001b[39m=\u001b[39m run_scf(name, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2552\u001b[0m \u001b[39mif\u001b[39;00m core\u001b[39m.\u001b[39mget_option(\u001b[39m'\u001b[39m\u001b[39mSCF\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mREFERENCE\u001b[39m\u001b[39m'\u001b[39m) \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mROHF\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCUHF\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/multisite/lib/python3.9/site-packages/psi4/driver/procrouting/proc.py:2455\u001b[0m, in \u001b[0;36mrun_scf\u001b[0;34m(name, **kwargs)\u001b[0m\n\u001b[1;32m   2453\u001b[0m     core\u001b[39m.\u001b[39mset_global_option(\u001b[39m'\u001b[39m\u001b[39mSCF_TYPE\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mDF\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 2455\u001b[0m scf_wfn \u001b[39m=\u001b[39m scf_helper(name, post_scf\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2456\u001b[0m returnvalue \u001b[39m=\u001b[39m scf_wfn\u001b[39m.\u001b[39menergy()\n",
      "File \u001b[0;32m~/miniconda3/envs/multisite/lib/python3.9/site-packages/psi4/driver/procrouting/proc.py:1670\u001b[0m, in \u001b[0;36mscf_helper\u001b[0;34m(name, post_scf, **kwargs)\u001b[0m\n\u001b[1;32m   1665\u001b[0m     scf_wfn\u001b[39m.\u001b[39mpe_state \u001b[39m=\u001b[39m solvent\u001b[39m.\u001b[39mpol_embed\u001b[39m.\u001b[39mCppeInterface(\n\u001b[1;32m   1666\u001b[0m         molecule\u001b[39m=\u001b[39mscf_molecule, options\u001b[39m=\u001b[39mpol_embed_options,\n\u001b[1;32m   1667\u001b[0m         basisset\u001b[39m=\u001b[39mscf_wfn\u001b[39m.\u001b[39mbasisset()\n\u001b[1;32m   1668\u001b[0m     )\n\u001b[0;32m-> 1670\u001b[0m e_scf \u001b[39m=\u001b[39m scf_wfn\u001b[39m.\u001b[39;49mcompute_energy()\n\u001b[1;32m   1671\u001b[0m \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m [core, scf_wfn]:\n\u001b[1;32m   1672\u001b[0m     \u001b[39m# set_variable(\"SCF TOTAL ENERGY\")  # P::e SCF\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/multisite/lib/python3.9/site-packages/psi4/driver/procrouting/scf_proc/scf_iterator.py:83\u001b[0m, in \u001b[0;36mscf_compute_energy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minitialize()\n\u001b[1;32m     85\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/multisite/lib/python3.9/site-packages/psi4/driver/procrouting/scf_proc/scf_iterator.py:193\u001b[0m, in \u001b[0;36mscf_initialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m mints \u001b[39m=\u001b[39m core\u001b[39m.\u001b[39mMintsHelper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbasisset())\n\u001b[0;32m--> 193\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minitialize_jk(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory_jk_, jk\u001b[39m=\u001b[39;49mjk)\n\u001b[1;32m    194\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mV_potential():\n",
      "File \u001b[0;32m~/miniconda3/envs/multisite/lib/python3.9/site-packages/psi4/driver/procrouting/scf_proc/scf_iterator.py:128\u001b[0m, in \u001b[0;36minitialize_jk\u001b[0;34m(self, memory, jk)\u001b[0m\n\u001b[1;32m    126\u001b[0m jk\u001b[39m.\u001b[39mset_omega_beta(functional\u001b[39m.\u001b[39mx_beta())   \n\u001b[0;32m--> 128\u001b[0m jk\u001b[39m.\u001b[39;49minitialize()\n\u001b[1;32m    129\u001b[0m jk\u001b[39m.\u001b[39mprint_header()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \nFatal Error: Matrix::power: C_DSYEV failed\nError occurred in file: /scratch/psilocaluser/conda-builds/psi4-multiout_1657298395608/work/psi4/src/psi4/libmints/matrix.cc on line: 2330\nThe most recent 5 function calls were:\n\npsi::FittingMetric::form_eig_inverse(double)\npsi::DiskDFJK::preiterations()\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/yxx/work/project/colmena/multisite_/evt/idea_simple_validate/idea_validate.ipynb Cell 5\u001b[0m line \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu_2/home/yxx/work/project/colmena/multisite_/evt/idea_simple_validate/idea_validate.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu_2/home/yxx/work/project/colmena/multisite_/evt/idea_simple_validate/idea_validate.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m to_run \u001b[39m=\u001b[39m task_queue_audit[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu_2/home/yxx/work/project/colmena/multisite_/evt/idea_simple_validate/idea_validate.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m diff_energy \u001b[39m=\u001b[39m simulation_task(to_run)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu_2/home/yxx/work/project/colmena/multisite_/evt/idea_simple_validate/idea_validate.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mend simulation at\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(time\u001b[39m.\u001b[39mtime()\u001b[39m-\u001b[39mstart_time))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu_2/home/yxx/work/project/colmena/multisite_/evt/idea_simple_validate/idea_validate.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# inference task\u001b[39;00m\n",
      "\u001b[1;32m/home/yxx/work/project/colmena/multisite_/evt/idea_simple_validate/idea_validate.ipynb Cell 5\u001b[0m line \u001b[0;36msimulation_task\u001b[0;34m(to_run, tempdir, num_threads)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu_2/home/yxx/work/project/colmena/multisite_/evt/idea_simple_validate/idea_validate.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m atoms\u001b[39m.\u001b[39mset_center_of_mass([\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu_2/home/yxx/work/project/colmena/multisite_/evt/idea_simple_validate/idea_validate.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m xyz \u001b[39m=\u001b[39m write_to_string(atoms, \u001b[39m'\u001b[39m\u001b[39mxyz\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu_2/home/yxx/work/project/colmena/multisite_/evt/idea_simple_validate/idea_validate.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m value \u001b[39m=\u001b[39m _run_calculator(xyz, calc, tempdir)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu_2/home/yxx/work/project/colmena/multisite_/evt/idea_simple_validate/idea_validate.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m atoms \u001b[39m=\u001b[39m read_from_string(value, \u001b[39m'\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu_2/home/yxx/work/project/colmena/multisite_/evt/idea_simple_validate/idea_validate.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m dft_energy \u001b[39m=\u001b[39m atoms\u001b[39m.\u001b[39mget_potential_energy()\n",
      "File \u001b[0;32m~/work/project/colmena/multisite_/finetuning-surrogates/fff/simulation/__init__.py:59\u001b[0m, in \u001b[0;36m_run_calculator\u001b[0;34m(xyz, calc, temp_path)\u001b[0m\n\u001b[1;32m     57\u001b[0m     atoms\u001b[39m.\u001b[39mget_potential_energy()\n\u001b[1;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCalculation failed: \u001b[39m\u001b[39m{\u001b[39;00mexc\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[39m# Convert it to JSON\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39mreturn\u001b[39;00m write_to_string(atoms, \u001b[39m'\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Calculation failed: \nFatal Error: Matrix::power: C_DSYEV failed\nError occurred in file: /scratch/psilocaluser/conda-builds/psi4-multiout_1657298395608/work/psi4/src/psi4/libmints/matrix.cc on line: 2330\nThe most recent 5 function calls were:\n\npsi::FittingMetric::form_eig_inverse(double)\npsi::DiskDFJK::preiterations()\n"
     ]
    }
   ],
   "source": [
    "## only run task\n",
    "# train task\n",
    "start_time = time.time()\n",
    "_1,_2=my_train_schnet(model_msg=StartModelMessage, train_data=train_set, valid_data=valid_sets[0])\n",
    "print(\"end train at\"+str(time.time()-start_time))\n",
    "\n",
    "# sampling task\n",
    "start_time = time.time()\n",
    "# 0~100 random array\n",
    "random_array = np.random.randint(0, 100, 100)\n",
    "starting_point = search_space[random_array[0]].starting\n",
    " \n",
    "audit, traj = my_run_dynamics(\n",
    "    atoms=starting_point, steps=run_length, calc=active_model_proxy)\n",
    "print(\"end sampling at\"+str(time.time()-start_time))\n",
    "\n",
    "# simulation task\n",
    "start_time = time.time()\n",
    "to_run = task_queue_audit[-1]\n",
    "diff_energy = simulation_task(to_run)\n",
    "print(\"end simulation at\"+str(time.time()-start_time))\n",
    "\n",
    "# inference task\n",
    "start_time = time.time()\n",
    "inference_batch = inference_pool[0:100]\n",
    "inference_proxies: list[None] = [None] * n_models\n",
    "inference_proxies[0] = SchnetCalculator(model_msgs[0])\n",
    "my_eval_schnet = _wrap(schnet.evaluate, device='cuda')\n",
    "my_eval_schnet(\"/home/yxx/work/project/colmena/multisite_/evt/idea_simple_validate/temp/model.pth\",inference_batch)\n",
    "print(\"end inference at\"+str(time.time()-start_time))\n",
    "\n",
    "## prepare task input data queue\n",
    "## append model\n",
    "# input: structure\n",
    "# output: model\n",
    "\n",
    "## append sampling data\n",
    "# input: model search space \n",
    "# output: task_queue_audit\n",
    "\n",
    "## append simulation data\n",
    "# input: task_queue_audit\n",
    "# output: diff_energy\n",
    "\n",
    "## append inference data\n",
    "# input: inference_pool\n",
    "# output: selected structure for trian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(12, 5.850254774093628), (15, 7.936183929443359), (18, 14.705122470855713), (21, 16.75410395860672), (24, 21.41688847541809), (27, 24.086347818374634), (30, 31.15443527698517), (33, 40.727830992804634), (36, 48.143983125686646), (39, 59.534986893335976), (42, 72.19054274559021), (45, 83.08321416378021), (48, 97.04547603925069), (51, 107.61373349598476), (54, 133.2091362476349), (57, 133.03949835896492), (60, 174.3393987417221), (63, 203.84687733650208), (66, 245.14138960838318), (69, 267.0610399246216), (72, 330.6697278022766), (75, 359.20263490080833)]\n",
      "{1: 459.49200105667114, 2: 308.5443150997162, 4: 178.95574569702148, 8: 114.76378226280212, 12: 88.14041328430176, 16: 76.19267272949219, 20: 68.74859023094177, 24: 64.68302941322327, 28: 63.44424271583557, 32: 66.51880431175232, 36: 69.2843005657196, 40: 70.93301844596863, 44: 69.64240193367004, 48: 70.88759207725525, 52: 69.63654398918152, 56: 71.50614309310913, 60: 71.06448984146118, 64: 72.96044158935547}\n"
     ]
    }
   ],
   "source": [
    "## total resources\n",
    "total_cpu = 64\n",
    "total_gpu = 4\n",
    "\n",
    "## task resources requirements(not accurate, experience)\n",
    "train_resources = {'cpu': 1, 'gpu': 1}\n",
    "sampling_resources = {'cpu': 1, 'gpu': 1}\n",
    "simulation_resources = {'cpu': 8, 'gpu': 0} # simulation cpu resources is not fixed, it can ranged from 1 to 16\n",
    "inference_resources = {'cpu': 1, 'gpu': 1}\n",
    "\n",
    "## total runs\n",
    "train_totals = 12\n",
    "sampling_totals = 60\n",
    "simulation_totals = 60\n",
    "inference_totals = 12\n",
    "\n",
    "## a batch runs\n",
    "train_nums = 4\n",
    "sampling_nums = 20\n",
    "simulation_nums = 20\n",
    "inference_nums = 4\n",
    "\n",
    "## running sequence\n",
    "### suppose i get sequence and nums from workflow\n",
    "tasks = []\n",
    "tasks.append({'name': 'train', 'resources': train_resources, 'nums': train_nums})\n",
    "tasks.append({'name': 'sampling', 'resources': sampling_resources, 'nums': sampling_nums})\n",
    "tasks.append({'name': 'simulation', 'resources': simulation_resources, 'nums': simulation_nums})\n",
    "tasks.append({'name': 'inference', 'resources': inference_resources, 'nums': inference_nums})\n",
    "\n",
    "## time consume\n",
    "trainning_time = 50\n",
    "sampling_time = 10\n",
    "inference_time = 10\n",
    "# simulation time\n",
    "## nonlinear performance for cpu cores\n",
    "## nonlinear preformance for atoms length\n",
    "with open(out_dir / 'length_time', 'rb') as fp:\n",
    "    length_time = pickle.load(fp)\n",
    "with open(out_dir / 'cpu_time', 'rb') as fp:\n",
    "    cpu_time = pickle.load(fp)\n",
    "print(length_time)\n",
    "print(cpu_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'sampling', 'nums': 20}, {'name': 'train', 'nums': 4}, {'name': 'train', 'nums': 4}, {'name': 'inference', 'nums': 4}]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# define tasks\n",
    "tasks = [{'name': 'train', 'nums': 4},\n",
    "         {'name': 'sampling', 'nums': 20},\n",
    "         {'name': 'simulation', 'nums': 20},\n",
    "         {'name': 'inference', 'nums': 4}]\n",
    "\n",
    "## estimate simulation running time\n",
    "def estimate_simulation_time(molecule_length, cpu_cores, length_times, core_times):\n",
    "    closest_length = min(length_times, key=lambda x:abs(x[0]-molecule_length))\n",
    "    length_time = closest_length[1]\n",
    "\n",
    "    closest_cores = min(core_times.keys(), key=lambda x:abs(x-cpu_cores))\n",
    "    core_time = core_times[closest_cores]\n",
    "\n",
    "    return length_time*core_time/100\n",
    "\n",
    "def calculate_resource_usage(task_allocation):\n",
    "    total_cpu_usage, total_gpu_usage = 0, 0\n",
    "    for task, resources in task_allocation.items():\n",
    "        total_cpu_usage += resources['cpu']\n",
    "        total_gpu_usage += resources['gpu']\n",
    "\n",
    "    return total_cpu_usage, total_gpu_usage\n",
    "\n",
    "def generate_population(population_size, tasks, total_cpu, total_gpu):\n",
    "    population = []\n",
    "    for _ in range(population_size):\n",
    "        individual = []\n",
    "        remaining_cpu, remaining_gpu = total_cpu, total_gpu\n",
    "\n",
    "        while remaining_cpu > 0 and remaining_gpu > 0:\n",
    "            task = random.choice(tasks)\n",
    "            cpu_usage = random.randint(1, min(remaining_cpu, task['resources']['cpu']))\n",
    "            gpu_usage = random.randint(0, min(remaining_gpu, task['resources']['gpu']))\n",
    "\n",
    "            individual.append({'name': task['name'], 'id': len(individual)+1, 'resources': {'cpu': cpu_usage, 'gpu': gpu_usage}})\n",
    "            remaining_cpu -= cpu_usage\n",
    "            remaining_gpu -= gpu_usage\n",
    "\n",
    "        population.append(individual)\n",
    "\n",
    "    return population\n",
    "\n",
    "def generate_individual():\n",
    "    return random.sample(tasks, len(tasks))\n",
    "\n",
    "def fitness(molecule_length, cpu_cores, length_times, core_times):\n",
    "    simulation_time = estimate_simulation_time(molecule_length, cpu_cores, length_times, core_times)\n",
    "    total_time = trainning_time + sampling_time + inference_time + simulation_time\n",
    "\n",
    "    score = 10000 / total_time # time is better to be smaller\n",
    "\n",
    "    return score\n",
    "\n",
    "def crossover(ind1, ind2):\n",
    "    size = min(len(ind1), len(ind2))\n",
    "    cxpoint = random.randint(1, size - 1)\n",
    "    ind1[cxpoint:], ind2[cxpoint:] = ind2[cxpoint:], ind1[cxpoint:]\n",
    "\n",
    "def mutate(individual):\n",
    "    mutate_point = random.randint(0, len(individual) - 1)\n",
    "    individual[mutate_point] = random.choice(tasks)\n",
    "\n",
    "def run_ga(pop_size, num_generations):\n",
    "    population = generate_population(pop_size)\n",
    "\n",
    "    for gen in range(num_generations):\n",
    "        scores = [fitness(ind) for ind in population]\n",
    "\n",
    "        population = [population[i] for i in np.argsort(scores)]\n",
    "\n",
    "        next_population = population[:pop_size//10]\n",
    "\n",
    "        for i in range(pop_size//10, pop_size):\n",
    "            if i < pop_size//2:\n",
    "                ind1, ind2 = random.sample(population[:pop_size//2], 2)\n",
    "                crossover(ind1, ind2)\n",
    "                next_population.append(ind1)\n",
    "            else:\n",
    "                ind = random.choice(population)\n",
    "                mutate(ind)\n",
    "                next_population.append(ind)\n",
    "        \n",
    "        population = next_population\n",
    "\n",
    "    return max(population, key=fitness)\n",
    "\n",
    "best_individual = run_ga(100, 100)\n",
    "print(best_individual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'train', 'nums': 4},\n",
       " {'name': 'inference', 'nums': 4},\n",
       " {'name': 'sampling', 'nums': 20},\n",
       " {'name': 'simulation', 'nums': 20}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multisite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
